{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Level Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import glob\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "import pprint\n",
    "import random\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import tarfile\n",
    "import time\n",
    "import unicodedata\n",
    "import urllib.request\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bundle_dataset(list):\n",
    "    files = [\n",
    "        {\n",
    "            'name': f,\n",
    "            'path': os.path.join(data_path, f),\n",
    "            'size': os.stat(os.path.join(data_path, f)).st_size,\n",
    "            'handle': None\n",
    "        }\n",
    "        for f in list\n",
    "    ]\n",
    "    total_size = reduce((lambda x, y: x + y), map((lambda x: x['size']), files))\n",
    "    return {\n",
    "        'files': files,\n",
    "        'total_size': total_size\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'files': [ { 'handle': None,\n",
      "               'name': 'cbtest_V_train.txt',\n",
      "               'path': 'data/CBTest/data/cbtest_V_train.txt',\n",
      "               'size': 247098043},\n",
      "             { 'handle': None,\n",
      "               'name': 'cbtest_V_test_2500ex.txt',\n",
      "               'path': 'data/CBTest/data/cbtest_V_test_2500ex.txt',\n",
      "               'size': 5686625},\n",
      "             { 'handle': None,\n",
      "               'name': 'cbtest_V_valid_2000ex.txt',\n",
      "               'path': 'data/CBTest/data/cbtest_V_valid_2000ex.txt',\n",
      "               'size': 4460425},\n",
      "             { 'handle': None,\n",
      "               'name': 'cbt_valid.txt',\n",
      "               'path': 'data/CBTest/data/cbt_valid.txt',\n",
      "               'size': 1182697},\n",
      "             { 'handle': None,\n",
      "               'name': 'cbtest_CN_train.txt',\n",
      "               'path': 'data/CBTest/data/cbtest_CN_train.txt',\n",
      "               'size': 295933246},\n",
      "             { 'handle': None,\n",
      "               'name': 'cbt_test.txt',\n",
      "               'path': 'data/CBTest/data/cbt_test.txt',\n",
      "               'size': 1528744},\n",
      "             { 'handle': None,\n",
      "               'name': 'cbtest_NE_train.txt',\n",
      "               'path': 'data/CBTest/data/cbtest_NE_train.txt',\n",
      "               'size': 248333387},\n",
      "             { 'handle': None,\n",
      "               'name': 'cbtest_CN_test_2500ex.txt',\n",
      "               'path': 'data/CBTest/data/cbtest_CN_test_2500ex.txt',\n",
      "               'size': 6018376},\n",
      "             { 'handle': None,\n",
      "               'name': 'cbtest_NE_test_2500ex.txt',\n",
      "               'path': 'data/CBTest/data/cbtest_NE_test_2500ex.txt',\n",
      "               'size': 5587722},\n",
      "             { 'handle': None,\n",
      "               'name': 'cbtest_P_valid_2000ex.txt',\n",
      "               'path': 'data/CBTest/data/cbtest_P_valid_2000ex.txt',\n",
      "               'size': 4680981},\n",
      "             { 'handle': None,\n",
      "               'name': 'cbtest_P_test_2500ex.txt',\n",
      "               'path': 'data/CBTest/data/cbtest_P_test_2500ex.txt',\n",
      "               'size': 5958048},\n",
      "             { 'handle': None,\n",
      "               'name': 'cbtest_CN_valid_2000ex.txt',\n",
      "               'path': 'data/CBTest/data/cbtest_CN_valid_2000ex.txt',\n",
      "               'size': 4641257},\n",
      "             { 'handle': None,\n",
      "               'name': 'cbt_train.txt',\n",
      "               'path': 'data/CBTest/data/cbt_train.txt',\n",
      "               'size': 25742364},\n",
      "             { 'handle': None,\n",
      "               'name': 'cbtest_P_train.txt',\n",
      "               'path': 'data/CBTest/data/cbtest_P_train.txt',\n",
      "               'size': 836819208},\n",
      "             { 'handle': None,\n",
      "               'name': 'cbtest_NE_valid_2000ex.txt',\n",
      "               'path': 'data/CBTest/data/cbtest_NE_valid_2000ex.txt',\n",
      "               'size': 4328316}],\n",
      "  'total_size': 1697999439}\n",
      "{ 'files': [ { 'handle': None,\n",
      "               'name': 'cbt_test.txt',\n",
      "               'path': 'data/CBTest/data/cbt_test.txt',\n",
      "               'size': 1528744}],\n",
      "  'total_size': 1528744}\n"
     ]
    }
   ],
   "source": [
    "data_path = os.path.join('data', 'CBTest', 'data')\n",
    "input_files =  os.listdir(data_path)\n",
    "\n",
    "all_dataset = bundle_dataset([f for f in input_files])\n",
    "train_dataset = bundle_dataset(['cbt_test.txt'])\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "pp.pprint(all_dataset)\n",
    "pp.pprint(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_letter = \"$\"\n",
    "_all_letters = \"abcdefghijklmnopqrstuvwxyz .,'\"\n",
    "all_letters = _all_letters + end_letter\n",
    "n_letters = len(all_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "def indexToLetter(index):\n",
    "    return all_letters[index]\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def textToTensor(text):\n",
    "    tensor = torch.zeros(len(text), 1, n_letters)\n",
    "    for li, letter in enumerate(text):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "def textToIndexTensor(text):\n",
    "    tensor = torch.zeros(len(text), 1, dtype=torch.long)\n",
    "    for li, letter in enumerate(text):\n",
    "        tensor[li][0] = letterToIndex(letter)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o'neal,,.\n"
     ]
    }
   ],
   "source": [
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "# O'Néàl => O'Neal\n",
    "def transform(s):\n",
    "    s = s.lower()\n",
    "    s = ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "    s = ' '.join(s.split())\n",
    "    s = s.replace(\n",
    "        ' .', '.'\n",
    "    ).replace(\n",
    "        ' ,', ','\n",
    "    )\n",
    "    return s\n",
    "\n",
    "print(transform(\"O'Néàl\\\":    , , .\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcdefghijklmnopqrstuvwxyz .,'$ 31\n",
      "0\n",
      "tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n",
      "tensor([[0],\n",
      "        [1]])\n"
     ]
    }
   ],
   "source": [
    "print(all_letters, n_letters)\n",
    "print(letterToIndex('a'))\n",
    "print(letterToTensor('b'))\n",
    "print(textToTensor('ab'))\n",
    "print(textToIndexTensor('ab'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "booktitle andrewlangtheyellowfairybook.txt.out\n"
     ]
    }
   ],
   "source": [
    "max_lines = 50\n",
    "min_size = 10\n",
    "\n",
    "def next_inputs(bundled_dataset):\n",
    "    files = bundled_dataset['files']\n",
    "    total_size = bundled_dataset['total_size']\n",
    "    \n",
    "    file_idx = np.random.choice(len(files), p = [f['size'] / total_size for f in files])\n",
    "    \n",
    "    if files[file_idx]['handle'] == None:\n",
    "        files[file_idx]['handle'] = open(files[file_idx]['path'], encoding='utf-8')\n",
    "    \n",
    "    cnt_lines = 0\n",
    "    text = ''\n",
    "    for i in range(max_lines):\n",
    "        line = files[file_idx]['handle'].readline()\n",
    "        if line == None:\n",
    "            files[file_idx]['handle'].close()\n",
    "            break\n",
    "        text += line.strip()\n",
    "        cnt_lines += 1\n",
    "\n",
    "        if len(text) >= min_size:\n",
    "            if random.random() < 0.5:\n",
    "                break\n",
    "    return transform(text)\n",
    "\n",
    "def close_dataset_files(bundled_dataset):\n",
    "    files = bundled_dataset['files']\n",
    "    \n",
    "    for f in files:\n",
    "        if f['handle'] is not None:\n",
    "            f['handle'].close()\n",
    "            f['handle'] = None\n",
    "\n",
    "print(next_inputs(train_dataset))\n",
    "close_dataset_files(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM init() 5600\n",
      "LSTM init() 6944\n"
     ]
    }
   ],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        print('LSTM init()', (input_size + output_size) * output_size * 4)\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.wf = nn.Linear(input_size + output_size, output_size)\n",
    "        self.wi = nn.Linear(input_size + output_size, output_size)\n",
    "        self.wc = nn.Linear(input_size + output_size, output_size)\n",
    "        self.wo = nn.Linear(input_size + output_size, output_size)\n",
    "\n",
    "    def forward(self, input, output_in, context_in):\n",
    "        input_combined = torch.cat((input, output_in), 1)\n",
    "        f = torch.sigmoid(self.wf(input_combined))\n",
    "        i = torch.sigmoid(self.wi(input_combined))\n",
    "        c = torch.tanh(self.wc(input_combined))\n",
    "        o = torch.sigmoid(self.wo(input_combined))\n",
    "        \n",
    "        ic = torch.mul(i,c)\n",
    "        \n",
    "        context_out = torch.mul(context_in, f) + ic\n",
    "        output_out = torch.mul(torch.tanh(context_out), o)\n",
    "\n",
    "        return context_out, output_out\n",
    "\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(MyNet, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm1 = LSTM(n_letters, hidden_size)\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.lstm2 = LSTM(hidden_size, n_letters)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input, output_in, hidden_in, context_in):\n",
    "        context1_out, hidden_out = self.lstm1(input, hidden_in, context_in[0][:self.hidden_size])\n",
    "        hidden_out = self.dropout(hidden_out)\n",
    "        context2_out, output = self.lstm2(hidden_out, output_in, context_in[0][self.hidden_size:])\n",
    "        output_out = self.softmax(output)\n",
    "        return output_out, hidden_out, torch.cat((context1_out, context2_out), 1)\n",
    "\n",
    "hidden_size = 25\n",
    "my_net = MyNet(n_letters, n_letters, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
    "n_iters = 50000\n",
    "print_every = 500\n",
    "plot_every = 500\n",
    "current_loss = 0\n",
    "all_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(my_net.parameters(), lr = learning_rate)\n",
    "\n",
    "def train(text):\n",
    "    context = torch.zeros(1, hidden_size + n_letters) # memory\n",
    "    hidden = torch.zeros(1, hidden_size)\n",
    "    output = torch.zeros(1, n_letters)\n",
    "    loss = torch.zeros(1)\n",
    "    text_tensor = textToTensor(text)\n",
    "    text_index_tensor = textToIndexTensor(text)\n",
    "    outputs = \"\"\n",
    "    n_iteration = len(text) - 1\n",
    "\n",
    "    my_net.zero_grad()\n",
    "\n",
    "    for i in range(n_iteration):\n",
    "        output, hidden, context = my_net(text_tensor[i], output, hidden, context)\n",
    "        _, idx = output.max(1)\n",
    "        c = all_letters[idx[0].data]\n",
    "        outputs += c\n",
    "        l = F.nll_loss(output, text_index_tensor[i + 1])\n",
    "        loss += l\n",
    "    \n",
    "    loss = loss / n_iteration\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "#     for p in lstm.parameters():\n",
    "#         p.data.add_(-learning_rate, p.grad.data)\n",
    "    optimizer.step()\n",
    "\n",
    "    return outputs, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(context=torch.zeros(1, hidden_size + n_letters)):\n",
    "    output_text = ''\n",
    "    hidden = torch.zeros(1, hidden_size)\n",
    "    output = torch.zeros(1, n_letters)\n",
    "    input_ = torch.zeros(1, n_letters)\n",
    "\n",
    "    n_max_len = 500\n",
    "\n",
    "    for i in range(n_max_len):\n",
    "        tmp_output, hidden, context = my_net(input_, output, hidden, context)\n",
    "        val,idx = torch.max(tmp_output, 1)\n",
    "        c = all_letters[idx[0].data]\n",
    "        output_text += c\n",
    "        if c == end_letter:\n",
    "            break\n",
    "        \n",
    "        output = tmp_output\n",
    "        input_ = textToTensor(c)[0]\n",
    "\n",
    "    return output_text, context\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500] 1% (0m 59s) 2.8371\n",
      "- TO BE: go back, ' she said, to the castle, and bury this little diamond close to the door.$\n",
      "- AS IS: h                                                                                   \n",
      "- Sample: e                            a     h                  a                            h       h         a     h          h   a                                h h               a h                                     t                   h        h  h          a          h                                            a                                          h           h   h             a            h         h                                 h            h   a      a       a   t                     \n",
      "1516.558836221695\n",
      "[1000] 2% (2m 17s) 2.8008\n",
      "- TO BE: impatient at not being recognised, fairerthanafairy now drew out her third present, and on opening the crystal scentbottle a little syren flew out, who silenced the violins and then sang close to the prince 's ear the story of all his lady love had suffered in her search for him.she added some gentle reproaches to her tale, but before she had got far he was wide awake, and transported with joy threw himself at the princess 's feet.$\n",
      "- AS IS: n                                                      a             h    t            t              t                   t        h          t    the           h  t           t    t          th       e           h   th               h    t    t                           e  t                                     h te                   t   t   h          t   t               the                        t       h      t        h   o     \n",
      "- Sample: e h  h  h t  h   h t  a t      a  th   h th   h    h   a  h   a t  h  h  th th  h t t  a  a t he t       h  a  h   a t an  a t   a t   th t h   t  h h  he  h  h   t     ha  t  t h h   a  a   a    a t  he   a t  h t an t   t    a t  a t  h   a   h  t he    h    a  h     h      a  th t  a  t h   h a   a  h    h   a   he   a      a    th  a t   a  a t th he  t   t  a th  t h a te  h  he t      a   a t   a t      th  t  h   a     h t  h        h    an t  h  h a   a t  a t  he  he t   t a    a   h a \n",
      "1402.232754945755\n",
      "[1500] 3% (3m 26s) 2.7064\n",
      "- TO BE: and so he too joined them and away the ship flew on, and on, and on, and once more the simpleton looked out, and this time he saw a man carrying straw upon his back. hallo where are you carrying that straw to '$\n",
      "- AS IS: n  tante thn taa    the  hn  an   the aaen toe  hn  an  tn  t   an  t   an   ta   ahe tanee   n tan e  tn h a   hhet thne aa th  anhet aa    ne aaaa  tt   te  tan e aan   taa e t   ta  tan      ahen ta e  ah t  \n",
      "- Sample: e the an the the an the the the an the the ta the tha th the ha the the the ha the the an the an the tha the th the an the h the the th a the the han the the an tan t an the an he ha th han the tan an the th a the the an the th an tha the the the a han ha ton the th the a t ha he the th a the a he an an the the the the ton the th an han to he he the a tha ton hea the the the thea t a the the the the han ha the the an hea the the th ha the the the the he the the the tha an the tha the an tha the \n",
      "1390.0717763900757\n",
      "[2000] 4% (4m 20s) 2.7271\n",
      "- TO BE: they went on travelling for two days through a great forest, without food or drink, and without coming across a single house, and every night they had to climb up into the trees through fear of the wild beasts that were in the wood.$\n",
      "- AS IS: he  thn  tn thete      tet thhoao   thee  h t toe   ho    h ho he   tot  tt tee    a   hathe   th     h het  h tott e he     tn  h e   tothe the  te  th too  ett hn   the the   thee   eto   tn hhe tot  to     toe hth e tt the ho n   \n",
      "- Sample: e he the the the th he the the the the the the the he the the the the the the the the the the a the the the the the the the the hea the the to the the to to the the the to the a the the the to the the he he the he ha to the tha the the at to the the the the the the the the ho the the the the the the the the the tot he the the the tha the heat he ha to the the to the the the the to to the the the the the to ho the the ton the hothe the the the the the the the an he the the to he the the th the th\n",
      "1380.6406848430634\n",
      "[2500] 5% (5m 19s) 2.7179\n",
      "- TO BE: all his colour had disappeared whether this had happened on his travels or whether it was the result of trouble, who can say he looked at the little lady, she looked at him, and he felt that he was melting but he remained steadfast, with his gun at his shoulder.$\n",
      "- AS IS: n  tan hhn    te  aot n e     aha     then aa  ae ee    tn he  hhe     hn tha  e  hn ho  hhe te e   tn the  e   hoe thn ha  te he     hnhthe ten  e ten   hhe han e  hn te   hn  he ha   ahe  te hhn te      hnt he heaen    teh    n   hanheae  han tn te  hha        \n",
      "- Sample: e ha the ha the the hen he the the he the the a he ha tha the he the tha han he a the an the the ha the the the the the ta he the the the the to the the the to he the tha ha the the the te th the the the the tha the ha an the the the the tha the the ha he he the the hen tha the an tha the tha the he the tan the the ton the the the an te the the the a the to the the an the the the the the the he tha the the the the he he the he the the the he the hea the he the an he the the he he he he the the t\n",
      "1370.499740600586\n",
      "[3000] 6% (6m 8s) 2.8091\n",
      "- TO BE: it was so large a house, that she did not like to go nearer till she had nibbled some more of the lefthand bit of mushroom, and raised herself to about two feet high even then she walked up towards it rather timidly, saying to herself suppose it should be raving mad after all$\n",
      "- AS IS:   ahn aa ae    a aa     ahe  aha aa   a   e   ah  a aa     ah   aoe ae  ae      aa   ae   an aha aa ah    aa  an aa e e    a   aa n   ae e    ah an    aha ae   ae   aae  aha  aae aa     a e h      a  ao  e  ao       aa     ah aa  a   aa      a  aaa    aa aa en  ae  a     a    \n",
      "- Sample: e a a a a a a a  a he an  a a a a a a a a a a a an a a a a a a a a a a a a a a a a a a a a a a a a a a a a an a a a an a a an an a a a an a a a a a a a  an a a a a a a a a a a a a a a a  a a ne a a a an a a a a a a an an a  a a a a a a a a a a a a a a a a a a a $\n",
      "1392.2339794635773\n",
      "[3500] 7% (6m 55s) 2.8451\n",
      "- TO BE: thundered uncle abimelech, as he flung the newspaper down on the table.murray got up and peered over.then he whistled.$\n",
      "- AS IS: he       a  ee a   e e he a  ae aoe  eaee ao  e     aa ee n ahe nh ee  e  e  ao  a  a   aeane  ane   h   he aae        \n",
      "- Sample: e a a a a an a a a a an a a a ne a a a an a a  a a a a a  an  an a ha a a an a an a a an a a a a  a a an a an a a a a a hea a a  a a a a  a a a  a a a a a a a a an an a a a a an a an a a a a a an a a  a he a a a a  an a a an an an an a a a an a  a a a a a a a ne a an a a  a a a an a no a a an a a a a a a a a a a a   an an a a an a a a a an an a a an a  a a a a a an a a an a an a a an a a a  an a a a a a $\n",
      "1399.9459130764008\n",
      "[4000] 8% (7m 34s) 2.7756\n",
      "- TO BE: she had fallen asleep with her head against the seatback. i 've got a basket over there, '' said aunt cyrilla firmly, and i 've some presents in it that i was taking to my nephew 's children.$\n",
      "- AS IS: ee aa  ae     a      ha   ae  aa   a h     ahe ah     h  ana$e ao  anae     a e  ahe    a  aan  a    aooe    aan  e  an  a a e ao e aoe      an a  ahe  anaan ah     ah ao aeaee  a  ahen  e    \n",
      "- Sample: e a a a a an a an a an a a a an a an an an a a a an a a a  an a a a a he a an a a ha he a a an a a a a a a a he an  an a an a a an a a a an an a an a a a a an an a he an an a a a an a a an a a  he an a an a a an an he an an an a a a an a an a a a a an a an a an a a an a a a a a an an a an a a a a an an a an a a an an an an an a a  an a a a an a he a an an a an a a a a he a en a a a a an a a a he an a a a hea a an a an an a a an an a  a an a a  a a a an a hea a a an a an a an a a a a an an an a a\n",
      "1410.7837204933167\n",
      "[4500] 9% (8m 17s) 2.8087\n",
      "- TO BE: the next day ned allen went down to see mr. dutcher, or old dutcher, as he was universally called in carleton.$\n",
      "- AS IS: he aeaa aa  ae  a     aoa  aao  aonae  aee aa  ee   an ao  ao  e    a  aa aa  a           ahn    an ao  e      \n",
      "- Sample: e a a a a an an an a an an a a a an a an an an an a an a a ha a an an an an a hea a a an an an an ha an a a an a a he nea a an a an a a an ne a an an an a a an a a a an an ha a a an an a a a an an an a an a an a na a an a a ne an a a a an an an an an a an an a a an an an ha an a a an  an an a a an a a  an an a a a a an a a a an a  a an he an an an a a he a an a an an a an a a an a an an a a an a a a na an a a a an an a an a an a a a ne he a he a a an a he an an an a an a an a he an an an an a an\n",
      "1402.3617680072784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5000] 10% (9m 1s) 2.7789\n",
      "- TO BE: the little room was flooded with a mellow light from the pinkglobed lamp on the table, and in the soft, shadowy radiance she was as beautiful as a dream.she wore a dress of crepe, cut low in the neck.$\n",
      "- AS IS: he he     aeo  ae  aee     aen  anae     aa    aoe eahe aon   e    ae eeanehee ah     a   a  hhe ao    aae     ao  n  h aae aan an aea       a  hnaee  e    aa   anaee   a  aoeaea aa  ae  a  ahe ae e   \n",
      "- Sample: o he an an  an an an an an a an a an an a a he an a he a an an an an a he an he a an a an he he an a he an a a ne a a an a an a a  a a an an a $\n",
      "1394.6834952831268\n",
      "[5500] 11% (9m 41s) 2.8105\n",
      "- TO BE: the next day mrs. elwell said, ches, abner stearns wants you to go up there for a fortnight while tom bixby is away, and drive the milk wagon of mornings and do the chores for mrs. stearns.$\n",
      "- AS IS: he ae   ao  aeee aa     ao    hhe   a     aae     ao    aa  ah aa a eahe   he  hnhe        aae   ahoeaen e a  a     a   aoe e ahe aen eae h  an ae e     a   ao hhe hae    ae eaoee aoo    e  \n",
      "- Sample: e he a he he an a an an a an he he an a a he an a an an a an an a an an an he an an an ah an an an a he a an he an ha he an ha an a an an an a an an an a a an a an a an an he a an a an an a he an he an a he an an he an a a an a he an a a an an a an an an an an a an he an he ane an an an a a an an a an a he an an an an he a an a a a an an a an a he an he an an an an a a an an a an an he an a he he ane an a an an a a an an a ne ne an an a a an a an an an an an a a he an a a an an a hea an an an an\n",
      "1395.7390921115875\n",
      "[6000] 12% (10m 24s) 2.7979\n",
      "- TO BE: he got his freedom and he married maud carroll in six months ' time.$\n",
      "- AS IS: e aen ae  aoaan   an  ae ae  e   ao   aan e   a  hen ae  ee a$ahne   \n",
      "- Sample: e a an a he an an an an a he a  a he he an he he an a an an  an an an an an he he he he an he a an an a an an an a he he he an he a a ha han a ne an a an an an a an a he a an an an an an a an an an na he an he an an a a an an a a ha a ne an a ha a he he an a an an a an an he a a he an an he an an a he an a an he a he a an an a an a he he an he a an an he he an he a an a he a he an he a an a he an an he an an a a an a  an an an an a a an a he an he an an he an he a an an  an an a an he an an an a\n",
      "1399.699026107788\n",
      "[6500] 13% (11m 11s) 2.7601\n",
      "- TO BE: this time, they said, they were really going to have a leader and become the wisest people in the jungle so wise that everyone else would notice and envy them.$\n",
      "- AS IS: he  ao    aoe  aon   a e  ao e ao     ao    ao ae e a ae     an  ae  ne a e ao     ae   e an aoe ae   e ae aon  aoe  aae      an   ao    ao     an  an   aoe    \n",
      "- Sample: o a a a a an an a a a an an a an an a a an an an an a an an an an a an a an a a an a a a an a a a an an a a  an a  an an a a a a a a an an an a a an a an an an an a an a a an an an an an an an a a  an a an a an an an an an an a a a a ne an a an an an an an an a a an an a a an an an an an a a  an a a an a a a an an an a a an an a an a an a an a an an an an a a a a an a an a  a a an an an an a a a a an a a a an an an an a an a an an an an a an an an a a an a a an an a a an a a a a a a an a a an a \n",
      "1395.219120979309\n",
      "[7000] 14% (11m 59s) 2.8330\n",
      "- TO BE: then she would take the straightest of straight lines in his direction, striking out with her fore flippers and knocking the youngsters head over heels right and left.$\n",
      "- AS IS: he  aoa aoo   ao e aoe  ooan  e    an ao e   e aon   an ae  aa   e     aooone   an  aen  ae  aa   aoenee e a   aoo e    aoa ao         ae   ane  ae    aen e a   aeae   \n",
      "- Sample: e a an a an a a a an a a an an a an an an an a a an an a an nea an an an an an a a a a a a an an a a an an an a an a an an an a an an an a an an a an a a an a a an an a an a an a an an an a a an an a a an an a a a a an a an an an  a a a a an a an an an an a a an a a a a an an a a a an a a a an a an a a a a an a an an an a no a an a an an a a a an an an an an  an an an a a an an an an a a an a e an an an an an an an an an an a an an an a an an a an a ne a a an a an a a an a an an a an a an  an a \n",
      "1407.648772239685\n",
      "[7500] 15% (12m 55s) 2.8002\n",
      "- TO BE: all this was done last night, and i have counted seventy tracks crossing the river.$\n",
      "- AS IS: n  aoen aa  aa   aa   ae  e  an  a ae e aa      aaae    aoe e  aoe      aoe ae e  $ \n",
      "- Sample: e a a a an a an an an an a a a  an an an an a an a an an an a an a an an an an an an an an an an an a an an an an an an a nea an a  an a an an an an an an an an an an an an an an a an an an a a an an an a an an an an a a an an a an an a a an a an an an an an an an a an a an a a an a an a an an a an an a a an a an an  an an an an a an an an a an an an an an an an an an an an a a a a an an an an an an an a an a a an an an a a an an a a a an an an a an an an an an a an an a an a an an a an a an a a\n",
      "1411.4950761795044\n",
      "[8000] 16% (13m 39s) 2.8154\n",
      "- TO BE: you 've guessed it, peter, '' said he. handy tongue, is n't it '' i think it 's a very queer tongue, '' retorted peter, and i do n't understand it at all.$\n",
      "- AS IS: o  a$e aa      a   aaa  e a$ ae   ae  ae    ao      an aoa a  a$ a aoe  ean a$ anae   ao    ao      a$ aa       aeae   an  anaa ae  an         a  a  an    \n",
      "- Sample: e  an a a an a an a an an an a an an a a an a a a an an an a an a  an an an a a a an an a  a an a a a a an an an an an a a a a an an a an a a a a a an a an a an an a a an a  an a a a a an a an a an an an a a an a an a an a an an a a  an a an an a a an a an an a nee an an an an a a an an a an an a a a an a a a an an a a an a a an a an a a an an an a an an a an a an a a an a a an a a an an an an a a an an an a an a a an a an an an a an an an an an a a a a an a a a a an an a a an an an a an a a an \n",
      "1412.501627445221\n",
      "[8500] 17% (14m 4s) 2.1281\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: e  a a an a  on ne an an an a an an an a an an an a ne an an an an an an an nean an an an an an an an a an an  an an an an an an an an an  an non an an an an a an an an an a a an an an an an ne an an an an an a an an an  an an a an an an an an an an nea na an an a an a an a an an an an an an an an an a an an an nee a an an an an an an an a an an an an an an an a an an an a an an an an a ne an an an an a an an a an an an an a an an an a an an an an an a an an an a ne an an a a ne an an an an  a a\n",
      "1359.426808834076\n",
      "[9000] 18% (14m 5s) 2.0841\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: ea a an an a an an an an  an an an a an an an an an an an  an a an a an an an an  on an an an a a an an an a an nea an an an an an an an a an a an a an an an an ne an a an an a an a an a a an an an an an an an an a a a an an a an an an an a an an an ne a ne an an an an an an an an a an an a an on an a no an a na an an a an a an  an an an an a an an nea an an an an an an an an an an an a an an an a an an an an a an a an an an an a na an an an an an an an an an an an an an an an an an an an an an \n",
      "1052.3897640705109\n",
      "[9500] 19% (14m 6s) 2.0528\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: e a no an an an a an an nan an a a  an an an an an an an none an an an an an a an an an a an an an an an an an an a a an an a an an an an an an an an an an an an an an an an an an an an an an an an ne an an an an a an non an a an no an an an an an  an an an an  on an a no a an an an a a an a an an a an an a an an an an an an an an an an an no an an an an no an an an an an on an a an an an  an an an an an an an an an an an a an an an an an an an an a an an an a non an an an an an a an an an an ne\n",
      "1030.988914489746\n",
      "[10000] 20% (14m 7s) 2.0528\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: eea an an an an an noo an an na non  an an an an an an an an an a an an no an an no an an an an an an an an an an an an an an an an an a  ne an an an an no an an an an an an an an an  an an an an an a  on an a an an an an an an an an an a an an an an an an an a no an no a non an an an a an an an an a an an an an an an an an an an an an an an an an an an a an an an an no an an an an an an an a nea an an an an an an an an a a ne an a an a an an an an a an an a an a an a an an an an an an an an an \n",
      "1027.1846516132355\n",
      "[10500] 21% (14m 8s) 2.0528\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1027.0124003887177\n",
      "[11000] 22% (14m 9s) 2.0528\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: ha an an a an an an an a an an non an an an an an an an an an a an an  a an a an non an an an a ne nean an an an an  an a a non an an an an an a an an an an na an a no a nea an nean an an a an an a an an an ne a an a an an an an an an an an an a nea an a an  an a an  an an an an an an an an an an a a an an an no an an  an an an an an a an an an an an an an an an an a nan an an an an an a an an an an an an an an a an an an a ne an an an an a an an an an an an  an a ne an an an an an an an a an a \n",
      "1027.0413837432861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11500] 23% (14m 10s) 2.0528\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: e an na an an an a an an an an an an an an an a  an nea  an an an an an an a  an an an an an a an non a nea ne an an an an an an an no an na an an an a an an an an an an an an an an an a an  an ne an an an an a an an no an a nean an an an an a an an an a  an an a an an an no an an an ne an an an an a an a a a an an  an a a  on a a an an  no an an an an  an a  an a an an an an an a no an an  an  an an an an an an an an a an a noon an a an nea an an an an  an non an an an an an an an an an an an a\n",
      "1026.615356206894\n",
      "[12000] 24% (14m 11s) 2.0529\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: he a an an an an an an an a an an an an an an an an an an an an a  an an an an an an an a  on an a an an nean a an ne an an a  an an an no a a an an an  an an an an a an an an an an an an  an an a  on a an an an an a a  an a an an an an a a no a an an  an a  an ne an an an a an an an an an an an an an a an an an an an an an an no nea a an an an an nee  one an a an no an an an an an an a  nean an an a an a a an a a non an en a an an na an a an a an an an a an an an  an an an an an an an a an a ne\n",
      "1026.5111918449402\n",
      "[12500] 25% (14m 12s) 2.0528\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: he an  an an a an an an an a an a an an an an a an an an an an a an an an an a an an an an an an an an an an an an an an an a an a an an an  an a an a  an an an an a an a an no an an an no an ne an a an a a an an an a an nea an a ne an an an an an a an an a an an an an an an an an a ne an an an an an an an a an  an a  an an an an an nea an an a a an an an a an a  on an an an an an an ne nea nea nean an an an an  oo an an an a an an an an an no an an an an nee an an an an an a an an an an a an an\n",
      "1026.7824988365173\n",
      "[13000] 26% (14m 13s) 2.0528\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1026.5195000171661\n",
      "[13500] 27% (14m 14s) 2.0528\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: ee ne an an an an an a  an an an an an an an a  an  an an a a  an an an an no a an  an a a an an an an no an an an an a an a an a  a no no an an an an an an an an a an an a an an an an an a a nean an an a a  an  nan an a no an an an nea an a na an an an an an an a nean an an an an a no ne an an an an an a an an an a a  an a a a ne an an a an an na an an  an an an an an a an a nean an no an an ne an an  a an an an an an a an na an an an an an an an an an an an an an an an an an an an an  an an an\n",
      "1026.498258113861\n",
      "[14000] 28% (14m 16s) 2.0528\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1026.4855041503906\n",
      "[14500] 28% (14m 17s) 2.0528\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: ho a  an an  an an an an an an an a an an an an a no on an a noo an an an  na no an an an an an an an an an an a an an an  an an an a an an an a an an an an a an an a an an an an  an an an an an an an non an an a  an an an an an an an an an an an an a an an a no an a an an an an a an an an an an an a an an na an an an a no an an an an a an an an an an an an an an an an an an an an a an ne an an a na an a a an a  an an an an an  an an an an a an an an a an a a  an a an an an nea  on an non an an \n",
      "1026.412991285324\n",
      "[15000] 30% (14m 18s) 2.0528\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1026.411731004715\n",
      "[15500] 31% (14m 19s) 2.0528\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: he an nee no a an an an a nean an a an non ne an an an a no an an an an an an a a an an an a an an an an a an an a ne an an an an an an an an an a an an an an a an an an an an an an an an ne an an an an an an an an a  on a an an an an an an an an a an a an an a ne an an an an an an an a an an  an an a an an ne a an a  on an a an an an a ne a an an a a  an an  nan an an an an an an an an an an a an a an a an an an an a an an an an a an an a an an an a ea a an an a nea a an an ne an a a nea an an \n",
      "1026.4062271118164\n",
      "[16000] 32% (14m 20s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1017.1142699718475\n",
      "[16500] 33% (14m 21s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1011.2497403621674\n",
      "[17000] 34% (14m 22s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.7481920719147\n",
      "[17500] 35% (14m 23s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1783759593964\n",
      "[18000] 36% (14m 24s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1616868972778\n",
      "[18500] 37% (14m 25s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1813781261444\n",
      "[19000] 38% (14m 26s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1593463420868\n",
      "[19500] 39% (14m 27s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.148731470108\n",
      "[20000] 40% (14m 28s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1562008857727\n",
      "[20500] 41% (14m 29s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1574647426605\n",
      "[21000] 42% (14m 30s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1460521221161\n",
      "[21500] 43% (14m 31s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1453399658203\n",
      "[22000] 44% (14m 32s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1428453922272\n",
      "[22500] 45% (14m 33s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.144722700119\n",
      "[23000] 46% (14m 34s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1428635120392\n",
      "[23500] 47% (14m 35s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1573851108551\n",
      "[24000] 48% (14m 36s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1428136825562\n",
      "[24500] 49% (14m 37s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.177241563797\n",
      "[25000] 50% (14m 38s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1433258056641\n",
      "[25500] 51% (14m 39s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1435351371765\n",
      "[26000] 52% (14m 40s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1423404216766\n",
      "[26500] 53% (14m 41s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1425292491913\n",
      "[27000] 54% (14m 42s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1428756713867\n",
      "[27500] 55% (14m 43s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1442058086395\n",
      "[28000] 56% (14m 44s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1421830654144\n",
      "[28500] 56% (14m 45s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1424672603607\n",
      "[29000] 57% (14m 46s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1423780918121\n",
      "[29500] 59% (14m 46s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1421928405762\n",
      "[30000] 60% (14m 47s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1425185203552\n",
      "[30500] 61% (14m 48s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1423363685608\n",
      "[31000] 62% (14m 49s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1422715187073\n",
      "[31500] 63% (14m 50s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1421241760254\n",
      "[32000] 64% (14m 51s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1423964500427\n",
      "[32500] 65% (14m 52s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1421163082123\n",
      "[33000] 66% (14m 53s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1421434879303\n",
      "[33500] 67% (14m 54s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1427612304688\n",
      "[34000] 68% (14m 55s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1422867774963\n",
      "[34500] 69% (14m 56s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.142094373703\n",
      "[35000] 70% (14m 57s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1421008110046\n",
      "[35500] 71% (14m 58s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1421492099762\n",
      "[36000] 72% (14m 58s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1421022415161\n",
      "[36500] 73% (14m 59s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1420965194702\n",
      "[37000] 74% (15m 1s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1421530246735\n",
      "[37500] 75% (15m 1s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1426963806152\n",
      "[38000] 76% (15m 2s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.142101764679\n",
      "[38500] 77% (15m 3s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.142115354538\n",
      "[39000] 78% (15m 4s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1421039104462\n",
      "[39500] 79% (15m 5s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.14208984375\n",
      "[40000] 80% (15m 6s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1420888900757\n",
      "[40500] 81% (15m 7s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1420907974243\n",
      "[41000] 82% (15m 8s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.142169713974\n",
      "[41500] 83% (15m 9s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.14208984375\n",
      "[42000] 84% (15m 10s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.142092704773\n",
      "[42500] 85% (15m 11s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1420903205872\n",
      "[43000] 86% (15m 12s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1420929431915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43500] 87% (15m 13s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1420891284943\n",
      "[44000] 88% (15m 14s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1420888900757\n",
      "[44500] 89% (15m 15s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1420893669128\n",
      "[45000] 90% (15m 16s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1421797275543\n",
      "[45500] 91% (15m 17s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.14209151268\n",
      "[46000] 92% (15m 18s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1420974731445\n",
      "[46500] 93% (15m 19s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1420893669128\n",
      "[47000] 94% (15m 20s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1421117782593\n",
      "[47500] 95% (15m 21s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1420912742615\n",
      "[48000] 96% (15m 21s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1420965194702\n",
      "[48500] 97% (15m 22s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.14208984375\n",
      "[49000] 98% (15m 23s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1420922279358\n",
      "[49500] 99% (15m 24s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1420891284943\n",
      "[50000] 100% (15m 25s) 2.0203\n",
      "- TO BE: $\n",
      "- AS IS: $\n",
      "- Sample: $\n",
      "1010.1420884132385\n",
      "[3.0331176724433897, 2.80446550989151, 2.7801435527801512, 2.761281369686127, 2.7409994812011718, 2.7844679589271544, 2.7998918261528014, 2.8215674409866334, 2.804723536014557, 2.7893669905662537, 2.791478184223175, 2.7993980522155764, 2.790438241958618, 2.81529754447937, 2.822990152359009, 2.825003254890442, 2.718853617668152, 2.1047795281410218, 2.0619778289794923, 2.054369303226471, 2.054024800777435, 2.0540827674865723, 2.0532307124137876, 2.05302238368988, 2.0535649976730346, 2.053039000034332, 2.052996516227722, 2.0529710083007813, 2.0528259825706483, 2.05282346200943, 2.052812454223633, 2.0342285399436952, 2.022499480724335, 2.021496384143829, 2.0203567519187926, 2.0203233737945556, 2.020362756252289, 2.0203186926841736, 2.020297462940216, 2.0203124017715455, 2.020314929485321, 2.0202921042442323, 2.0202906799316405, 2.0202856907844544, 2.020289445400238, 2.020285727024078, 2.0203147702217104, 2.020285627365112, 2.020354483127594, 2.020286651611328, 2.020287070274353, 2.0202846808433534, 2.0202850584983825, 2.020285751342773, 2.0202884116172792, 2.020284366130829, 2.0202849345207214, 2.0202847561836244, 2.0202843856811525, 2.0202850370407104, 2.0202846727371218, 2.0202845430374143, 2.0202842483520507, 2.0202847929000853, 2.0202842326164245, 2.0202842869758606, 2.0202855224609375, 2.0202845735549926, 2.020284188747406, 2.020284201622009, 2.0202842984199525, 2.020284204483032, 2.0202841930389406, 2.020284306049347, 2.0202853927612305, 2.020284203529358, 2.020284230709076, 2.0202842078208922, 2.0202841796875, 2.0202841777801512, 2.0202841815948487, 2.020284339427948, 2.0202841796875, 2.0202841854095457, 2.020284180641174, 2.020284185886383, 2.0202841782569885, 2.0202841777801512, 2.0202841787338257, 2.0202843594551085, 2.02028418302536, 2.020284194946289, 2.0202841787338257, 2.0202842235565184, 2.020284182548523, 2.0202841930389406, 2.0202841796875, 2.0202841844558717, 2.0202841782569885, 2.020284176826477]\n"
     ]
    }
   ],
   "source": [
    "current_loss = 0\n",
    "for iter in range(1, n_iters + 1):\n",
    "    text = next_inputs(train_dataset) + end_letter\n",
    "    if text == '':\n",
    "        continue;\n",
    "    outputs, loss = train(text + end_letter)\n",
    "    current_loss += loss\n",
    "\n",
    "    # Print iter number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        print('[%d] %d%% (%s) %.4f' % (iter, iter / n_iters * 100, timeSince(start), loss))\n",
    "        print('- TO BE: %s' % text)\n",
    "        print('- AS IS: %s' % outputs)\n",
    "        print('- Sample: %s' % sample()[0])\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        print(current_loss)\n",
    "        current_loss = 0\n",
    "\n",
    "print(all_losses)\n",
    "close_dataset_files(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$\n",
      "$\n",
      "$\n",
      "$\n",
      "$\n",
      "$\n",
      "$\n",
      "$\n",
      "$\n",
      "$\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros(1, hidden_size + n_letters)\n",
    "for i in range(10):\n",
    "    output_text, context = sample(context=context)\n",
    "    print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html\n",
    "- x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
