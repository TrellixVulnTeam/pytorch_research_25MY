{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Level Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import glob\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "import pprint\n",
    "import random\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import tarfile\n",
    "import time\n",
    "import unicodedata\n",
    "import urllib.request\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bundle_dataset(list):\n",
    "    files = [\n",
    "        {\n",
    "            'name': f,\n",
    "            'path': os.path.join(data_path, f),\n",
    "            'size': os.stat(os.path.join(data_path, f)).st_size,\n",
    "            'handle': None\n",
    "        }\n",
    "        for f in list\n",
    "    ]\n",
    "    total_size = reduce((lambda x, y: x + y), map((lambda x: x['size']), files))\n",
    "    return {\n",
    "        'files': files,\n",
    "        'total_size': total_size\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'files': [ { 'handle': None,\n",
      "               'name': 'cbt_test.txt',\n",
      "               'path': 'data/CBTest/data/cbt_test.txt',\n",
      "               'size': 1528744},\n",
      "             { 'handle': None,\n",
      "               'name': 'cbt_train.txt',\n",
      "               'path': 'data/CBTest/data/cbt_train.txt',\n",
      "               'size': 25742364},\n",
      "             { 'handle': None,\n",
      "               'name': 'cbt_valid.txt',\n",
      "               'path': 'data/CBTest/data/cbt_valid.txt',\n",
      "               'size': 1182697},\n",
      "             { 'handle': None,\n",
      "               'name': 'cbtest_CN_test_2500ex.txt',\n",
      "               'path': 'data/CBTest/data/cbtest_CN_test_2500ex.txt',\n",
      "               'size': 6018376},\n",
      "             { 'handle': None,\n",
      "               'name': 'cbtest_CN_train.txt',\n",
      "               'path': 'data/CBTest/data/cbtest_CN_train.txt',\n",
      "               'size': 295933246},\n",
      "             { 'handle': None,\n",
      "               'name': 'cbtest_CN_valid_2000ex.txt',\n",
      "               'path': 'data/CBTest/data/cbtest_CN_valid_2000ex.txt',\n",
      "               'size': 4641257},\n",
      "             { 'handle': None,\n",
      "               'name': 'cbtest_NE_test_2500ex.txt',\n",
      "               'path': 'data/CBTest/data/cbtest_NE_test_2500ex.txt',\n",
      "               'size': 5587722},\n",
      "             { 'handle': None,\n",
      "               'name': 'cbtest_NE_train.txt',\n",
      "               'path': 'data/CBTest/data/cbtest_NE_train.txt',\n",
      "               'size': 248333387},\n",
      "             { 'handle': None,\n",
      "               'name': 'cbtest_NE_valid_2000ex.txt',\n",
      "               'path': 'data/CBTest/data/cbtest_NE_valid_2000ex.txt',\n",
      "               'size': 4328316},\n",
      "             { 'handle': None,\n",
      "               'name': 'cbtest_P_test_2500ex.txt',\n",
      "               'path': 'data/CBTest/data/cbtest_P_test_2500ex.txt',\n",
      "               'size': 5958048},\n",
      "             { 'handle': None,\n",
      "               'name': 'cbtest_P_train.txt',\n",
      "               'path': 'data/CBTest/data/cbtest_P_train.txt',\n",
      "               'size': 836819208},\n",
      "             { 'handle': None,\n",
      "               'name': 'cbtest_P_valid_2000ex.txt',\n",
      "               'path': 'data/CBTest/data/cbtest_P_valid_2000ex.txt',\n",
      "               'size': 4680981},\n",
      "             { 'handle': None,\n",
      "               'name': 'cbtest_V_test_2500ex.txt',\n",
      "               'path': 'data/CBTest/data/cbtest_V_test_2500ex.txt',\n",
      "               'size': 5686625},\n",
      "             { 'handle': None,\n",
      "               'name': 'cbtest_V_train.txt',\n",
      "               'path': 'data/CBTest/data/cbtest_V_train.txt',\n",
      "               'size': 247098043},\n",
      "             { 'handle': None,\n",
      "               'name': 'cbtest_V_valid_2000ex.txt',\n",
      "               'path': 'data/CBTest/data/cbtest_V_valid_2000ex.txt',\n",
      "               'size': 4460425}],\n",
      "  'total_size': 1697999439}\n",
      "{ 'files': [ { 'handle': None,\n",
      "               'name': 'cbt_test.txt',\n",
      "               'path': 'data/CBTest/data/cbt_test.txt',\n",
      "               'size': 1528744}],\n",
      "  'total_size': 1528744}\n"
     ]
    }
   ],
   "source": [
    "data_path = os.path.join('data', 'CBTest', 'data')\n",
    "input_files =  os.listdir(data_path)\n",
    "\n",
    "all_dataset = bundle_dataset([f for f in input_files])\n",
    "train_dataset = bundle_dataset(['cbt_test.txt'])\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "pp.pprint(all_dataset)\n",
    "pp.pprint(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_letters = \"abcdefghijklmnopqrstuvwxyz .,'\"\n",
    "n_letters = len(all_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o'neal,,.\n"
     ]
    }
   ],
   "source": [
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "# O'Néàl => O'Neal\n",
    "def transform(s):\n",
    "    s = s.lower()\n",
    "    s = ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "    s = ' '.join(s.split())\n",
    "    s = s.replace(\n",
    "        ' .', '.'\n",
    "    ).replace(\n",
    "        ' ,', ','\n",
    "    )\n",
    "    return s\n",
    "\n",
    "print(transform(\"O'Néàl\\\":    , , .\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def textToTensor(text):\n",
    "    tensor = torch.zeros(len(text), 1, n_letters)\n",
    "    for li, letter in enumerate(text):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "def textToIndexTensor(text):\n",
    "    tensor = torch.zeros(len(text), 1, dtype=torch.long)\n",
    "    for li, letter in enumerate(text):\n",
    "        tensor[li][0] = letterToIndex(letter)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcdefghijklmnopqrstuvwxyz .,' 30\n",
      "0\n",
      "tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n",
      "tensor([[0],\n",
      "        [1]])\n"
     ]
    }
   ],
   "source": [
    "print(all_letters, len(all_letters))\n",
    "print(letterToIndex('a'))\n",
    "print(letterToTensor('b'))\n",
    "print(textToTensor('ab'))\n",
    "print(textToIndexTensor('ab'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "booktitle andrewlangtheyellowfairybook.txt.out\n"
     ]
    }
   ],
   "source": [
    "max_lines = 100\n",
    "min_size = 20\n",
    "\n",
    "def next_inputs(bundled_dataset):\n",
    "    files = bundled_dataset['files']\n",
    "    total_size = bundled_dataset['total_size']\n",
    "    \n",
    "    file_idx = np.random.choice(len(files), p = [f['size'] / total_size for f in files])\n",
    "    \n",
    "    if files[file_idx]['handle'] == None:\n",
    "        files[file_idx]['handle'] = open(files[file_idx]['path'], encoding='utf-8')\n",
    "    \n",
    "    cnt_lines = 0\n",
    "    text = ''\n",
    "    for i in range(max_lines):\n",
    "        line = files[file_idx]['handle'].readline()\n",
    "        if line == None:\n",
    "            files[file_idx]['handle'].close()\n",
    "            break\n",
    "        text += line.strip()\n",
    "        cnt_lines += 1\n",
    "\n",
    "        if len(text) >= min_size:\n",
    "            if random.random() < 0.5:\n",
    "                break\n",
    "    return transform(text)\n",
    "\n",
    "def close_dataset_files(bundled_dataset):\n",
    "    files = bundled_dataset['files']\n",
    "    \n",
    "    for f in files:\n",
    "        if f['handle'] is not None:\n",
    "            f['handle'].close()\n",
    "            f['handle'] = None\n",
    "\n",
    "print(next_inputs(train_dataset))\n",
    "close_dataset_files(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM init() 11200\n",
      "LSTM init() 8400\n"
     ]
    }
   ],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        print('LSTM init()', (input_size + output_size) * output_size * 4)\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.wf = nn.Linear(input_size + output_size, output_size)\n",
    "        self.wi = nn.Linear(input_size + output_size, output_size)\n",
    "        self.wc = nn.Linear(input_size + output_size, output_size)\n",
    "        self.wo = nn.Linear(input_size + output_size, output_size)\n",
    "\n",
    "    def forward(self, input, output_in, context_in):\n",
    "#         print(input.size(), output_in.size(), context_in.size())\n",
    "        input_combined = torch.cat((input, output_in), 1)\n",
    "        f = torch.sigmoid(self.wf(input_combined))\n",
    "        i = torch.sigmoid(self.wi(input_combined))\n",
    "        c = torch.tanh(self.wc(input_combined))\n",
    "        o = torch.sigmoid(self.wo(input_combined))\n",
    "        \n",
    "        ic = torch.mul(i,c)\n",
    "        \n",
    "        context_out = torch.mul(context_in, f) + ic\n",
    "        output_out = torch.mul(torch.tanh(context_out), o)\n",
    "\n",
    "        return context_out, output_out\n",
    "\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(MyNet, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm1 = LSTM(n_letters, hidden_size)\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.lstm2 = LSTM(hidden_size, n_letters)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input, output_in, hidden_in, context_in):\n",
    "#         print(context_in[:self.hidden_size].size(), self.hidden_size, context_in.size())\n",
    "        context1_out, hidden_out = self.lstm1(input, hidden_in, context_in[0][:self.hidden_size])\n",
    "        hidden_out = self.dropout(hidden_out)\n",
    "        context2_out, output = self.lstm2(hidden_out, output_in, context_in[0][self.hidden_size:])\n",
    "        output_out = self.softmax(output)\n",
    "        return output_out, hidden_out, torch.cat((context1_out, context2_out), 1)\n",
    "\n",
    "hidden_size = 40\n",
    "my_net = MyNet(n_letters, n_letters, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
    "n_iters = 30000\n",
    "print_every = 100\n",
    "plot_every = 100\n",
    "current_loss = 0\n",
    "all_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(my_net.parameters(), lr = learning_rate)\n",
    "\n",
    "def train(text):\n",
    "    context = torch.zeros(1, hidden_size + n_letters)\n",
    "    hidden = torch.zeros(1, hidden_size)\n",
    "    output = torch.zeros(1, n_letters)\n",
    "    loss = torch.zeros(1)\n",
    "    text_tensor = textToTensor(text)\n",
    "    text_index_tensor = textToIndexTensor(text)\n",
    "    outputs = \"\"\n",
    "    n_iteration = len(text) - 1\n",
    "\n",
    "    my_net.zero_grad()\n",
    "\n",
    "    for i in range(n_iteration):\n",
    "        output, hidden, context = my_net(text_tensor[i], output, hidden, context)\n",
    "        _, idx = output.max(1)\n",
    "        c = all_letters[idx]\n",
    "        outputs += c\n",
    "        l = F.nll_loss(output, text_index_tensor[i + 1])\n",
    "        loss += l\n",
    "    \n",
    "    loss = loss / n_iteration\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "#     for p in lstm.parameters():\n",
    "#         p.data.add_(-learning_rate, p.grad.data)\n",
    "    optimizer.step()\n",
    "\n",
    "    return outputs, loss.item(), my_net.parameters()\n",
    "\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "current_loss = 0\n",
    "for iter in range(1, n_iters + 1):\n",
    "    text = next_inputs(train_dataset)\n",
    "    if text == '':\n",
    "        continue;\n",
    "    outputs, loss, params = train(text)\n",
    "    current_loss += loss\n",
    "\n",
    "    # Print iter number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        print('[%d] %d%% (%s) %.4f' % (iter, iter / n_iters * 100, timeSince(start), loss))\n",
    "        print('- TO BE: %s' % text)\n",
    "        print('- AS IS: %s' % outputs)\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        print(current_loss)\n",
    "        current_loss = 0\n",
    "#         for p in params:\n",
    "#             print('- PARAMS: %s' % p)\n",
    "print(all_losses)\n",
    "close_dataset_files(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html\n",
    "- x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
